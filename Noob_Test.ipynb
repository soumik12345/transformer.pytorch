{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from src.model import Transformer\n",
    "from src.utils import subsequent_mask\n",
    "from src.training import Batch, NoamOptimizer, LabelSmoothing, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(\n",
    "            np.random.randint(\n",
    "                1, V, size=(batch, 10)\n",
    "            )\n",
    "        )\n",
    "        data[:, 0] = 1\n",
    "        src = torch.autograd.Variable(data, requires_grad=False)\n",
    "        tgt = torch.autograd.Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(\n",
    "            x.contiguous().view(-1, x.size(-1)),\n",
    "            y.contiguous().view(-1)\n",
    "        ) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumikrakshit/Workspace/transformer.pytorch/src/model.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(p)\n",
      "/usr/local/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "model = Transformer(V, V, n=2)\n",
    "criterion = LabelSmoothing(\n",
    "    size=V, padding_index=0, smoothing=0.0\n",
    ")\n",
    "model_opt = NoamOptimizer(\n",
    "    model.source_embedding[0].d_model, 1, 400,\n",
    "    torch.optim.Adam(\n",
    "        model.parameters(), lr=0,\n",
    "        betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 3.222836 Tokens per Sec: 356.105133\n",
      "Epoch Step: 1 Loss: 1.898097 Tokens per Sec: 514.855530\n",
      "tensor(1.8864)\n",
      "Epoch Step: 1 Loss: 1.961129 Tokens per Sec: 477.678619\n",
      "Epoch Step: 1 Loss: 1.615044 Tokens per Sec: 545.999451\n",
      "tensor(1.6368)\n",
      "Epoch Step: 1 Loss: 1.881133 Tokens per Sec: 312.640778\n",
      "Epoch Step: 1 Loss: 1.423026 Tokens per Sec: 550.668030\n",
      "tensor(1.4335)\n",
      "Epoch Step: 1 Loss: 1.788222 Tokens per Sec: 462.523651\n",
      "Epoch Step: 1 Loss: 1.237994 Tokens per Sec: 498.938354\n",
      "tensor(1.2300)\n",
      "Epoch Step: 1 Loss: 1.650148 Tokens per Sec: 359.106323\n",
      "Epoch Step: 1 Loss: 0.960283 Tokens per Sec: 471.688324\n",
      "tensor(0.9734)\n",
      "Epoch Step: 1 Loss: 1.151660 Tokens per Sec: 358.550964\n",
      "Epoch Step: 1 Loss: 0.588864 Tokens per Sec: 339.864563\n",
      "tensor(0.6103)\n",
      "Epoch Step: 1 Loss: 0.920910 Tokens per Sec: 410.786011\n",
      "Epoch Step: 1 Loss: 0.376575 Tokens per Sec: 511.768738\n",
      "tensor(0.4024)\n",
      "Epoch Step: 1 Loss: 0.554213 Tokens per Sec: 342.275574\n",
      "Epoch Step: 1 Loss: 0.228428 Tokens per Sec: 427.981476\n",
      "tensor(0.2461)\n",
      "Epoch Step: 1 Loss: 0.494119 Tokens per Sec: 141.509064\n",
      "Epoch Step: 1 Loss: 0.220053 Tokens per Sec: 370.060181\n",
      "tensor(0.1928)\n",
      "Epoch Step: 1 Loss: 0.487329 Tokens per Sec: 343.877258\n",
      "Epoch Step: 1 Loss: 0.125596 Tokens per Sec: 419.193542\n",
      "tensor(0.1224)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_step(\n",
    "        data_gen(V, 30, 20), model, \n",
    "        SimpleLossCompute(\n",
    "            model.generator,\n",
    "            criterion, model_opt\n",
    "        )\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\n",
    "        train_step(\n",
    "            data_gen(V, 30, 5), model, \n",
    "            SimpleLossCompute(\n",
    "                model.generator,\n",
    "                criterion, None\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, max_length, start_symbol):\n",
    "    memory = model.encode(source, source_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(source.data)\n",
    "    for i in range(max_length - 1):\n",
    "        out = model.decode(\n",
    "            memory, source_mask,\n",
    "            torch.autograd.Variable(ys), \n",
    "            torch.autograd.Variable(\n",
    "                subsequent_mask(ys.size(1)).type_as(source.data)\n",
    "            )\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.ones(1, 1).type_as(source.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "source = torch.autograd.Variable(\n",
    "    torch.LongTensor(\n",
    "        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "    )\n",
    ")\n",
    "source_mask = torch.autograd.Variable(\n",
    "    torch.ones(1, 1, 10)\n",
    ")\n",
    "print(\n",
    "    greedy_decode(\n",
    "        model, source, source_mask,\n",
    "        max_length=10, start_symbol=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
